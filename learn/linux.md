# Linux é«˜æ€§èƒ½å¹¶å‘ç¼–ç¨‹æŒ‡å—

## ğŸ“‹ ç›®å½•
- [å¤šçº¿ç¨‹å¹¶å‘æŠ€æœ¯](#å¤šçº¿ç¨‹å¹¶å‘æŠ€æœ¯)
- [å¤šè¿›ç¨‹å¹¶å‘æŠ€æœ¯](#å¤šè¿›ç¨‹å¹¶å‘æŠ€æœ¯)
- [Linuxç‰¹æœ‰é«˜æ€§èƒ½æœºåˆ¶](#linuxç‰¹æœ‰é«˜æ€§èƒ½æœºåˆ¶)
- [æ— é”ç¼–ç¨‹æŠ€æœ¯](#æ— é”ç¼–ç¨‹æŠ€æœ¯)
- [å†…å­˜ç®¡ç†ä¸ä¼˜åŒ–](#å†…å­˜ç®¡ç†ä¸ä¼˜åŒ–)
- [æ€§èƒ½è°ƒä¼˜æŠ€å·§](#æ€§èƒ½è°ƒä¼˜æŠ€å·§)
- [å®æˆ˜æ¡ˆä¾‹ä¸åŸºå‡†æµ‹è¯•](#å®æˆ˜æ¡ˆä¾‹ä¸åŸºå‡†æµ‹è¯•)

---

## ğŸ§µ å¤šçº¿ç¨‹å¹¶å‘æŠ€æœ¯

### **1. æ ‡å‡†çº¿ç¨‹åº“ (C++11/14/17/20)**

#### **åŸºç¡€çº¿ç¨‹æ“ä½œ**
```cpp
#include <thread>
#include <iostream>
#include <vector>
#include <chrono>

// é«˜æ€§èƒ½çº¿ç¨‹åˆ›å»º
class HighPerformanceThreadPool {
private:
    std::vector<std::thread> workers;
    std::queue<std::function<void()>> tasks;
    std::mutex queue_mutex;
    std::condition_variable condition;
    bool stop;

public:
    HighPerformanceThreadPool(size_t threads) : stop(false) {
        // æ ¹æ®CPUæ ¸å¿ƒæ•°åˆ›å»ºçº¿ç¨‹
        for (size_t i = 0; i < threads; ++i) {
            workers.emplace_back([this] {
                // è®¾ç½®CPUäº²å’Œæ€§
                cpu_set_t cpuset;
                CPU_ZERO(&cpuset);
                CPU_SET(sched_getcpu(), &cpuset);
                pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &cpuset);
                
                for (;;) {
                    std::function<void()> task;
                    {
                        std::unique_lock<std::mutex> lock(this->queue_mutex);
                        this->condition.wait(lock, [this] { return this->stop || !this->tasks.empty(); });
                        if (this->stop && this->tasks.empty()) return;
                        task = std::move(this->tasks.front());
                        this->tasks.pop();
                    }
                    task();
                }
            });
        }
    }
};
```

#### **C++20 é«˜æ€§èƒ½ç‰¹æ€§**
```cpp
#include <coroutine>
#include <semaphore>
#include <barrier>
#include <latch>
#include <jthread>

// C++20 åç¨‹ - é›¶æ‹·è´å¼‚æ­¥I/O
struct AsyncTask {
    struct promise_type {
        AsyncTask get_return_object() { return {}; }
        std::suspend_never initial_suspend() { return {}; }
        std::suspend_never final_suspend() noexcept { return {}; }
        void return_void() {}
        void unhandled_exception() {}
    };
};

AsyncTask async_network_io() {
    // ä½¿ç”¨ io_uring è¿›è¡Œå¼‚æ­¥I/O
    co_await schedule_io_operation();
    co_return;
}

// C++20 åŒæ­¥åŸè¯­ - é«˜æ€§èƒ½å±éšœ
class HighPerformanceBarrier {
    std::barrier<> sync_point;
public:
    HighPerformanceBarrier(std::ptrdiff_t count) : sync_point(count) {}
    
    void wait() {
        sync_point.arrive_and_wait();
    }
};

// C++20 ä¿¡å·é‡ - èµ„æºæ± ç®¡ç†
class ResourcePool {
    std::counting_semaphore<1000> available_resources{1000};
    
public:
    void acquire_resource() {
        available_resources.acquire();  // é˜»å¡ç›´åˆ°èµ„æºå¯ç”¨
    }
    
    void release_resource() {
        available_resources.release();
    }
};
```

### **2. POSIX çº¿ç¨‹ (pthread) - åº•å±‚æ§åˆ¶**

#### **é«˜æ€§èƒ½çº¿ç¨‹å±æ€§è®¾ç½®**
```cpp
#include <pthread.h>
#include <sched.h>
#include <sys/mman.h>

class OptimizedPThread {
public:
    static pthread_t create_high_performance_thread(void* (*start_routine)(void*), void* arg) {
        pthread_t thread;
        pthread_attr_t attr;
        
        // åˆå§‹åŒ–çº¿ç¨‹å±æ€§
        pthread_attr_init(&attr);
        
        // è®¾ç½®ä¸ºåˆ†ç¦»çŠ¶æ€ - é¿å…å†…å­˜æ³„æ¼
        pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED);
        
        // è®¾ç½®æ ˆå¤§å° - å‡å°‘å†…å­˜å¼€é”€
        size_t stack_size = 1024 * 1024;  // 1MBæ ˆ
        pthread_attr_setstacksize(&attr, stack_size);
        
        // è®¾ç½®è°ƒåº¦ç­–ç•¥ - å®æ—¶ä¼˜å…ˆçº§
        struct sched_param param;
        param.sched_priority = 50;
        pthread_attr_setschedpolicy(&attr, SCHED_FIFO);
        pthread_attr_setschedparam(&attr, &param);
        pthread_attr_setinheritsched(&attr, PTHREAD_EXPLICIT_SCHED);
        
        pthread_create(&thread, &attr, start_routine, arg);
        pthread_attr_destroy(&attr);
        
        return thread;
    }
    
    // CPUäº²å’Œæ€§è®¾ç½®
    static void set_cpu_affinity(pthread_t thread, int cpu_id) {
        cpu_set_t cpuset;
        CPU_ZERO(&cpuset);
        CPU_SET(cpu_id, &cpuset);
        pthread_setaffinity_np(thread, sizeof(cpu_set_t), &cpuset);
    }
};
```

#### **é«˜æ€§èƒ½äº’æ–¥é”**
```cpp
#include <pthread.h>

class FastMutex {
private:
    pthread_mutex_t mutex;
    
public:
    FastMutex() {
        pthread_mutexattr_t attr;
        pthread_mutexattr_init(&attr);
        
        // è®¾ç½®ä¸ºé€‚åº”æ€§äº’æ–¥é” - çŸ­æ—¶é—´è‡ªæ—‹ï¼Œé•¿æ—¶é—´é˜»å¡
        pthread_mutexattr_settype(&attr, PTHREAD_MUTEX_ADAPTIVE_NP);
        
        // è®¾ç½®è¿›ç¨‹é—´å…±äº« (å¦‚æœéœ€è¦)
        // pthread_mutexattr_setpshared(&attr, PTHREAD_PROCESS_SHARED);
        
        pthread_mutex_init(&mutex, &attr);
        pthread_mutexattr_destroy(&attr);
    }
    
    ~FastMutex() {
        pthread_mutex_destroy(&mutex);
    }
    
    void lock() { pthread_mutex_lock(&mutex); }
    void unlock() { pthread_mutex_unlock(&mutex); }
    bool try_lock() { return pthread_mutex_trylock(&mutex) == 0; }
};

// è¯»å†™é” - è¯»å¤šå†™å°‘åœºæ™¯ä¼˜åŒ–
class HighPerformanceRWLock {
private:
    pthread_rwlock_t rwlock;
    
public:
    HighPerformanceRWLock() {
        pthread_rwlockattr_t attr;
        pthread_rwlockattr_init(&attr);
        
        // ä¼˜å…ˆå†™è€…ç­–ç•¥
        pthread_rwlockattr_setkind_np(&attr, PTHREAD_RWLOCK_PREFER_WRITER_NONRECURSIVE_NP);
        
        pthread_rwlock_init(&rwlock, &attr);
        pthread_rwlockattr_destroy(&attr);
    }
    
    void read_lock() { pthread_rwlock_rdlock(&rwlock); }
    void write_lock() { pthread_rwlock_wrlock(&rwlock); }
    void unlock() { pthread_rwlock_unlock(&rwlock); }
};
```

### **3. çº¿ç¨‹æœ¬åœ°å­˜å‚¨ (TLS) ä¼˜åŒ–**

```cpp
#include <thread>

// C++11 thread_local - ç¼–è¯‘å™¨ä¼˜åŒ–
class ThreadLocalCache {
    static thread_local char buffer[4096];  // æ¯çº¿ç¨‹4KBç¼“å†²åŒº
    static thread_local size_t buffer_pos;
    
public:
    static void* fast_allocate(size_t size) {
        if (buffer_pos + size <= sizeof(buffer)) {
            void* ptr = buffer + buffer_pos;
            buffer_pos += size;
            return ptr;
        }
        return malloc(size);  // å›é€€åˆ°å †åˆ†é…
    }
};

// POSIX TLS - æ‰‹åŠ¨æ§åˆ¶
class PosixTLS {
private:
    static pthread_key_t key;
    static pthread_once_t key_once;
    
    static void make_key() {
        pthread_key_create(&key, destructor);
    }
    
    static void destructor(void* ptr) {
        free(ptr);
    }
    
public:
    static void* get_data() {
        pthread_once(&key_once, make_key);
        return pthread_getspecific(key);
    }
    
    static void set_data(void* data) {
        pthread_once(&key_once, make_key);
        pthread_setspecific(key, data);
    }
};
```

---

## ğŸ”„ å¤šè¿›ç¨‹å¹¶å‘æŠ€æœ¯

### **1. è¿›ç¨‹åˆ›å»ºä¸ç®¡ç†**

#### **é«˜æ€§èƒ½è¿›ç¨‹æ± **
```cpp
#include <sys/wait.h>
#include <unistd.h>
#include <signal.h>

class ProcessPool {
private:
    std::vector<pid_t> workers;
    int pipe_fd[2];
    
public:
    ProcessPool(size_t num_processes) {
        pipe(pipe_fd);
        
        for (size_t i = 0; i < num_processes; ++i) {
            pid_t pid = fork();
            if (pid == 0) {
                // å­è¿›ç¨‹
                worker_process();
                exit(0);
            } else if (pid > 0) {
                workers.push_back(pid);
            }
        }
    }
    
    void worker_process() {
        // è®¾ç½®è¿›ç¨‹ä¼˜å…ˆçº§
        setpriority(PRIO_PROCESS, 0, -10);
        
        // ç»‘å®šCPUæ ¸å¿ƒ
        cpu_set_t cpuset;
        CPU_ZERO(&cpuset);
        CPU_SET(getpid() % sysconf(_SC_NPROCESSORS_ONLN), &cpuset);
        sched_setaffinity(0, sizeof(cpuset), &cpuset);
        
        // å·¥ä½œå¾ªç¯
        while (true) {
            // å¤„ç†ä»»åŠ¡
            process_task();
        }
    }
    
    ~ProcessPool() {
        for (pid_t pid : workers) {
            kill(pid, SIGTERM);
            waitpid(pid, nullptr, 0);
        }
    }
};
```

### **2. é«˜æ€§èƒ½è¿›ç¨‹é—´é€šä¿¡ (IPC)**

#### **POSIX å…±äº«å†…å­˜ - é›¶æ‹·è´é€šä¿¡**
```cpp
#include <sys/mman.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <semaphore.h>

template<typename T>
class HighPerformanceSharedMemory {
private:
    int shm_fd;
    T* data;
    sem_t* read_sem;
    sem_t* write_sem;
    size_t size;
    
public:
    HighPerformanceSharedMemory(const char* name, size_t count) : size(sizeof(T) * count) {
        // åˆ›å»ºå…±äº«å†…å­˜
        shm_fd = shm_open(name, O_CREAT | O_RDWR, 0666);
        ftruncate(shm_fd, size);
        
        // æ˜ å°„åˆ°è¿›ç¨‹åœ°å€ç©ºé—´
        data = static_cast<T*>(mmap(nullptr, size, PROT_READ | PROT_WRITE, MAP_SHARED, shm_fd, 0));
        
        // ä½¿ç”¨å¤§é¡µé¢ä¼˜åŒ– (éœ€è¦rootæƒé™æˆ–é…ç½®)
        madvise(data, size, MADV_HUGEPAGE);
        
        // åˆ›å»ºä¿¡å·é‡
        read_sem = sem_open((std::string(name) + "_read").c_str(), O_CREAT, 0666, 0);
        write_sem = sem_open((std::string(name) + "_write").c_str(), O_CREAT, 0666, count);
    }
    
    // ç”Ÿäº§è€…å†™å…¥
    void write(const T& item, size_t index) {
        sem_wait(write_sem);        // ç­‰å¾…å¯å†™ç©ºé—´
        
        data[index] = item;         // é›¶æ‹·è´å†™å…¥
        __sync_synchronize();       // å†…å­˜å±éšœ
        
        sem_post(read_sem);         // é€šçŸ¥å¯è¯»
    }
    
    // æ¶ˆè´¹è€…è¯»å–
    T read(size_t index) {
        sem_wait(read_sem);         // ç­‰å¾…å¯è¯»æ•°æ®
        
        T result = data[index];     // é›¶æ‹·è´è¯»å–
        __sync_synchronize();       // å†…å­˜å±éšœ
        
        sem_post(write_sem);        // é€šçŸ¥å¯å†™
        return result;
    }
    
    ~HighPerformanceSharedMemory() {
        munmap(data, size);
        close(shm_fd);
        sem_close(read_sem);
        sem_close(write_sem);
    }
};
```

#### **POSIX æ¶ˆæ¯é˜Ÿåˆ— - é«˜ååé‡**
```cpp
#include <mqueue.h>

class HighThroughputMessageQueue {
private:
    mqd_t mq;
    struct mq_attr attr;
    
public:
    HighThroughputMessageQueue(const char* name, size_t max_msgs = 1000, size_t msg_size = 8192) {
        attr.mq_flags = 0;
        attr.mq_maxmsg = max_msgs;
        attr.mq_msgsize = msg_size;
        attr.mq_curmsgs = 0;
        
        mq = mq_open(name, O_CREAT | O_RDWR | O_NONBLOCK, 0666, &attr);
        if (mq == -1) {
            throw std::runtime_error("Failed to create message queue");
        }
    }
    
    // éé˜»å¡å‘é€ - é«˜æ€§èƒ½
    bool try_send(const void* msg_ptr, size_t msg_len, unsigned int msg_prio = 0) {
        return mq_send(mq, static_cast<const char*>(msg_ptr), msg_len, msg_prio) == 0;
    }
    
    // æ‰¹é‡å‘é€ä¼˜åŒ–
    size_t batch_send(const std::vector<std::pair<const void*, size_t>>& messages) {
        size_t sent = 0;
        for (const auto& [msg_ptr, msg_len] : messages) {
            if (try_send(msg_ptr, msg_len)) {
                ++sent;
            } else {
                break;  // é˜Ÿåˆ—æ»¡ï¼Œåœæ­¢å‘é€
            }
        }
        return sent;
    }
    
    // éé˜»å¡æ¥æ”¶
    ssize_t try_receive(void* msg_ptr, size_t msg_len, unsigned int* msg_prio = nullptr) {
        return mq_receive(mq, static_cast<char*>(msg_ptr), msg_len, msg_prio);
    }
    
    ~HighThroughputMessageQueue() {
        mq_close(mq);
    }
};
```

#### **ç®¡é“ä¼˜åŒ– - é«˜å¸¦å®½ä¼ è¾“**
```cpp
#include <sys/socket.h>
#include <fcntl.h>

class HighBandwidthPipe {
private:
    int pipe_fd[2];
    
public:
    HighBandwidthPipe() {
        // ä½¿ç”¨ socketpair ä»£æ›¿ pipe - åŒå‘é€šä¿¡
        if (socketpair(AF_UNIX, SOCK_STREAM, 0, pipe_fd) == -1) {
            throw std::runtime_error("Failed to create socketpair");
        }
        
        // è®¾ç½®ä¸ºéé˜»å¡
        fcntl(pipe_fd[0], F_SETFL, O_NONBLOCK);
        fcntl(pipe_fd[1], F_SETFL, O_NONBLOCK);
        
        // å¢å¤§ç¼“å†²åŒº
        int buffer_size = 1024 * 1024;  // 1MB
        setsockopt(pipe_fd[0], SOL_SOCKET, SO_RCVBUF, &buffer_size, sizeof(buffer_size));
        setsockopt(pipe_fd[1], SOL_SOCKET, SO_SNDBUF, &buffer_size, sizeof(buffer_size));
    }
    
    // é›¶æ‹·è´å‘é€ (ä½¿ç”¨ splice)
    ssize_t splice_send(int src_fd, size_t len) {
        return splice(src_fd, nullptr, pipe_fd[1], nullptr, len, SPLICE_F_MOVE);
    }
    
    // æ‰¹é‡å†™å…¥
    ssize_t writev(const struct iovec* iov, int iovcnt) {
        return ::writev(pipe_fd[1], iov, iovcnt);
    }
    
    // æ‰¹é‡è¯»å–
    ssize_t readv(const struct iovec* iov, int iovcnt) {
        return ::readv(pipe_fd[0], iov, iovcnt);
    }
    
    int get_read_fd() const { return pipe_fd[0]; }
    int get_write_fd() const { return pipe_fd[1]; }
    
    ~HighBandwidthPipe() {
        close(pipe_fd[0]);
        close(pipe_fd[1]);
    }
};
```

---

## âš¡ Linuxç‰¹æœ‰é«˜æ€§èƒ½æœºåˆ¶

### **1. epoll - é«˜å¹¶å‘I/Oå¤šè·¯å¤ç”¨**

```cpp
#include <sys/epoll.h>
#include <sys/eventfd.h>

class HighPerformanceEpoll {
private:
    int epoll_fd;
    struct epoll_event* events;
    int max_events;
    
public:
    HighPerformanceEpoll(int max_events = 1000) : max_events(max_events) {
        // åˆ›å»º epoll å®ä¾‹
        epoll_fd = epoll_create1(EPOLL_CLOEXEC);
        events = new struct epoll_event[max_events];
    }
    
    // æ·»åŠ æ–‡ä»¶æè¿°ç¬¦ - è¾¹ç¼˜è§¦å‘æ¨¡å¼
    void add_fd(int fd, uint32_t events_mask = EPOLLIN | EPOLLET) {
        struct epoll_event ev;
        ev.events = events_mask;
        ev.data.fd = fd;
        epoll_ctl(epoll_fd, EPOLL_CTL_ADD, fd, &ev);
    }
    
    // é«˜æ€§èƒ½äº‹ä»¶å¾ªç¯
    void event_loop() {
        while (true) {
            int nfds = epoll_wait(epoll_fd, events, max_events, -1);
            
            for (int i = 0; i < nfds; ++i) {
                if (events[i].events & EPOLLIN) {
                    handle_read(events[i].data.fd);
                }
                if (events[i].events & EPOLLOUT) {
                    handle_write(events[i].data.fd);
                }
                if (events[i].events & EPOLLERR) {
                    handle_error(events[i].data.fd);
                }
            }
        }
    }
    
private:
    void handle_read(int fd) { /* å¤„ç†è¯»äº‹ä»¶ */ }
    void handle_write(int fd) { /* å¤„ç†å†™äº‹ä»¶ */ }
    void handle_error(int fd) { /* å¤„ç†é”™è¯¯äº‹ä»¶ */ }
    
public:
    ~HighPerformanceEpoll() {
        delete[] events;
        close(epoll_fd);
    }
};
```

### **2. io_uring - å¼‚æ­¥I/Oæœ€æ–°æŠ€æœ¯**

```cpp
#include <liburing.h>

class IOUringAsyncIO {
private:
    struct io_uring ring;
    constexpr static int QUEUE_DEPTH = 256;
    
public:
    IOUringAsyncIO() {
        // åˆå§‹åŒ– io_uring
        if (io_uring_queue_init(QUEUE_DEPTH, &ring, 0) < 0) {
            throw std::runtime_error("Failed to initialize io_uring");
        }
    }
    
    // å¼‚æ­¥è¯»å–
    void async_read(int fd, void* buf, size_t len, off_t offset, void* user_data) {
        struct io_uring_sqe* sqe = io_uring_get_sqe(&ring);
        io_uring_prep_read(sqe, fd, buf, len, offset);
        io_uring_sqe_set_data(sqe, user_data);
        io_uring_submit(&ring);
    }
    
    // å¼‚æ­¥å†™å…¥
    void async_write(int fd, const void* buf, size_t len, off_t offset, void* user_data) {
        struct io_uring_sqe* sqe = io_uring_get_sqe(&ring);
        io_uring_prep_write(sqe, fd, buf, len, offset);
        io_uring_sqe_set_data(sqe, user_data);
        io_uring_submit(&ring);
    }
    
    // æ‰¹é‡æäº¤ - å‡å°‘ç³»ç»Ÿè°ƒç”¨
    void submit_batch() {
        io_uring_submit(&ring);
    }
    
    // ç­‰å¾…å®Œæˆ
    int wait_completion(void** user_data) {
        struct io_uring_cqe* cqe;
        int ret = io_uring_wait_cqe(&ring, &cqe);
        if (ret == 0) {
            *user_data = io_uring_cqe_get_data(cqe);
            int result = cqe->res;
            io_uring_cqe_seen(&ring, cqe);
            return result;
        }
        return ret;
    }
    
    ~IOUringAsyncIO() {
        io_uring_queue_exit(&ring);
    }
};
```

### **3. eventfd - è½»é‡çº§äº‹ä»¶é€šçŸ¥**

```cpp
#include <sys/eventfd.h>

class EventNotifier {
private:
    int event_fd;
    
public:
    EventNotifier() {
        event_fd = eventfd(0, EFD_CLOEXEC | EFD_NONBLOCK);
        if (event_fd == -1) {
            throw std::runtime_error("Failed to create eventfd");
        }
    }
    
    // å‘é€é€šçŸ¥
    void notify(uint64_t value = 1) {
        write(event_fd, &value, sizeof(value));
    }
    
    // æ¥æ”¶é€šçŸ¥
    uint64_t wait() {
        uint64_t value;
        read(event_fd, &value, sizeof(value));
        return value;
    }
    
    int get_fd() const { return event_fd; }
    
    ~EventNotifier() {
        close(event_fd);
    }
};

// è·¨è¿›ç¨‹äº‹ä»¶é€šçŸ¥ç³»ç»Ÿ
class CrossProcessEventSystem {
private:
    std::shared_ptr<HighPerformanceSharedMemory<EventNotifier>> shared_notifiers;
    
public:
    CrossProcessEventSystem(const char* name, size_t num_events) {
        shared_notifiers = std::make_shared<HighPerformanceSharedMemory<EventNotifier>>(name, num_events);
    }
    
    void notify_process(int process_id, uint64_t event_data) {
        shared_notifiers->write(EventNotifier(), process_id);
    }
};
```

### **4. timerfd - é«˜ç²¾åº¦å®šæ—¶å™¨**

```cpp
#include <sys/timerfd.h>

class HighPrecisionTimer {
private:
    int timer_fd;
    
public:
    HighPrecisionTimer() {
        timer_fd = timerfd_create(CLOCK_MONOTONIC, TFD_CLOEXEC | TFD_NONBLOCK);
        if (timer_fd == -1) {
            throw std::runtime_error("Failed to create timerfd");
        }
    }
    
    // è®¾ç½®å‘¨æœŸæ€§å®šæ—¶å™¨ - çº³ç§’çº§ç²¾åº¦
    void set_periodic(long interval_ns) {
        struct itimerspec timer_spec;
        timer_spec.it_interval.tv_sec = interval_ns / 1000000000;
        timer_spec.it_interval.tv_nsec = interval_ns % 1000000000;
        timer_spec.it_value = timer_spec.it_interval;
        
        timerfd_settime(timer_fd, 0, &timer_spec, nullptr);
    }
    
    // ç­‰å¾…å®šæ—¶å™¨è§¦å‘
    uint64_t wait() {
        uint64_t expirations;
        read(timer_fd, &expirations, sizeof(expirations));
        return expirations;
    }
    
    int get_fd() const { return timer_fd; }
    
    ~HighPrecisionTimer() {
        close(timer_fd);
    }
};
```

---

## ğŸ”’ æ— é”ç¼–ç¨‹æŠ€æœ¯

### **1. åŸå­æ“ä½œä¸å†…å­˜åº**

```cpp
#include <atomic>

// é«˜æ€§èƒ½æ— é”è®¡æ•°å™¨
class LockFreeCounter {
private:
    std::atomic<uint64_t> counter{0};
    
public:
    // åŸå­é€’å¢ - æœ€å¿«
    uint64_t increment() {
        return counter.fetch_add(1, std::memory_order_relaxed);
    }
    
    // åŸå­é€’å¢ - è·å–-ä¿®æ”¹-å­˜å‚¨è¯­ä¹‰
    uint64_t increment_acq_rel() {
        return counter.fetch_add(1, std::memory_order_acq_rel);
    }
    
    // åŸå­æ¯”è¾ƒäº¤æ¢
    bool compare_exchange(uint64_t expected, uint64_t desired) {
        return counter.compare_exchange_weak(expected, desired, std::memory_order_acq_rel);
    }
    
    uint64_t load() const {
        return counter.load(std::memory_order_acquire);
    }
};

// æ— é”æ ˆ - Michael & Scott ç®—æ³•
template<typename T>
class LockFreeStack {
private:
    struct Node {
        T data;
        std::atomic<Node*> next;
        Node(T data) : data(std::move(data)), next(nullptr) {}
    };
    
    std::atomic<Node*> head{nullptr};
    
public:
    void push(T item) {
        Node* new_node = new Node(std::move(item));
        new_node->next = head.load(std::memory_order_relaxed);
        
        while (!head.compare_exchange_weak(new_node->next, new_node,
                                          std::memory_order_release,
                                          std::memory_order_relaxed)) {
            // CAS å¤±è´¥ï¼Œé‡è¯•
        }
    }
    
    bool pop(T& result) {
        Node* old_head = head.load(std::memory_order_acquire);
        
        while (old_head && !head.compare_exchange_weak(old_head, old_head->next,
                                                      std::memory_order_release,
                                                      std::memory_order_relaxed)) {
            // CAS å¤±è´¥ï¼Œé‡è¯•
        }
        
        if (old_head) {
            result = std::move(old_head->data);
            delete old_head;
            return true;
        }
        return false;
    }
};
```

### **2. æ— é”é˜Ÿåˆ— - é«˜ååé‡**

```cpp
// å•ç”Ÿäº§è€…å•æ¶ˆè´¹è€…æ— é”é˜Ÿåˆ— - æœ€é«˜æ€§èƒ½
template<typename T, size_t SIZE>
class SPSCLockFreeQueue {
private:
    static_assert((SIZE & (SIZE - 1)) == 0, "SIZE must be power of 2");
    
    struct alignas(64) {  // é¿å…false sharing
        std::atomic<size_t> head{0};
    };
    
    struct alignas(64) {
        std::atomic<size_t> tail{0};
    };
    
    T buffer[SIZE];
    
public:
    // ç”Ÿäº§è€…å…¥é˜Ÿ
    bool enqueue(const T& item) {
        size_t current_tail = tail.load(std::memory_order_relaxed);
        size_t next_tail = (current_tail + 1) & (SIZE - 1);
        
        if (next_tail == head.load(std::memory_order_acquire)) {
            return false;  // é˜Ÿåˆ—æ»¡
        }
        
        buffer[current_tail] = item;
        tail.store(next_tail, std::memory_order_release);
        return true;
    }
    
    // æ¶ˆè´¹è€…å‡ºé˜Ÿ
    bool dequeue(T& item) {
        size_t current_head = head.load(std::memory_order_relaxed);
        
        if (current_head == tail.load(std::memory_order_acquire)) {
            return false;  // é˜Ÿåˆ—ç©º
        }
        
        item = buffer[current_head];
        head.store((current_head + 1) & (SIZE - 1), std::memory_order_release);
        return true;
    }
    
    size_t size() const {
        size_t h = head.load(std::memory_order_relaxed);
        size_t t = tail.load(std::memory_order_relaxed);
        return (t - h) & (SIZE - 1);
    }
};

// å¤šç”Ÿäº§è€…å¤šæ¶ˆè´¹è€…æ— é”é˜Ÿåˆ—
template<typename T>
class MPMCLockFreeQueue {
private:
    struct Node {
        std::atomic<T*> data{nullptr};
        std::atomic<Node*> next{nullptr};
    };
    
    std::atomic<Node*> head;
    std::atomic<Node*> tail;
    
public:
    MPMCLockFreeQueue() {
        Node* dummy = new Node;
        head.store(dummy);
        tail.store(dummy);
    }
    
    void enqueue(T item) {
        Node* new_node = new Node;
        T* data = new T(std::move(item));
        new_node->data.store(data);
        
        while (true) {
            Node* last = tail.load();
            Node* next = last->next.load();
            
            if (last == tail.load()) {
                if (next == nullptr) {
                    if (last->next.compare_exchange_weak(next, new_node)) {
                        break;
                    }
                } else {
                    tail.compare_exchange_weak(last, next);
                }
            }
        }
        tail.compare_exchange_weak(tail.load(), new_node);
    }
    
    bool dequeue(T& result) {
        while (true) {
            Node* first = head.load();
            Node* last = tail.load();
            Node* next = first->next.load();
            
            if (first == head.load()) {
                if (first == last) {
                    if (next == nullptr) {
                        return false;  // é˜Ÿåˆ—ç©º
                    }
                    tail.compare_exchange_weak(last, next);
                } else {
                    T* data = next->data.load();
                    if (data == nullptr) {
                        continue;
                    }
                    if (head.compare_exchange_weak(first, next)) {
                        result = *data;
                        delete data;
                        delete first;
                        return true;
                    }
                }
            }
        }
    }
};
```

### **3. å†…å­˜å›æ”¶ - Hazard Pointer**

```cpp
// å±é™©æŒ‡é’ˆ - å®‰å…¨å†…å­˜å›æ”¶
template<typename T>
class HazardPointer {
private:
    static thread_local T* hazard_ptr;
    static std::atomic<T*> retired_list;
    
public:
    static void protect(T* ptr) {
        hazard_ptr = ptr;
        std::atomic_thread_fence(std::memory_order_seq_cst);
    }
    
    static void clear() {
        hazard_ptr = nullptr;
    }
    
    static void retire(T* ptr) {
        // å°†æŒ‡é’ˆåŠ å…¥é€€ä¼‘åˆ—è¡¨
        ptr->next_retired = retired_list.load();
        while (!retired_list.compare_exchange_weak(ptr->next_retired, ptr)) {
            // é‡è¯•
        }
        
        // å®šæœŸæ¸…ç†
        if (should_cleanup()) {
            cleanup_retired();
        }
    }
    
private:
    static void cleanup_retired() {
        // æ‰«ææ‰€æœ‰çº¿ç¨‹çš„å±é™©æŒ‡é’ˆ
        std::set<T*> hazard_ptrs = collect_hazard_pointers();
        
        // å›æ”¶ä¸åœ¨å±é™©æŒ‡é’ˆä¸­çš„å¯¹è±¡
        T* current = retired_list.exchange(nullptr);
        while (current) {
            T* next = current->next_retired;
            if (hazard_ptrs.find(current) == hazard_ptrs.end()) {
                delete current;
            } else {
                retire(current);  // é‡æ–°åŠ å…¥é€€ä¼‘åˆ—è¡¨
            }
            current = next;
        }
    }
};
```

---

## ğŸ’¾ å†…å­˜ç®¡ç†ä¸ä¼˜åŒ–

### **1. å†…å­˜æ± æŠ€æœ¯**

```cpp
#include <sys/mman.h>

// é«˜æ€§èƒ½å†…å­˜æ± 
class HighPerformanceMemoryPool {
private:
    struct Block {
        Block* next;
    };
    
    void* memory_start;
    size_t block_size;
    size_t total_size;
    Block* free_list;
    std::atomic<size_t> allocated_count{0};
    
public:
    HighPerformanceMemoryPool(size_t block_size, size_t num_blocks) 
        : block_size(std::max(block_size, sizeof(Block))), 
          total_size(this->block_size * num_blocks) {
        
        // ä½¿ç”¨ mmap åˆ†é…å¤§é¡µå†…å­˜
        memory_start = mmap(nullptr, total_size, PROT_READ | PROT_WRITE,
                           MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
        
        if (memory_start == MAP_FAILED) {
            throw std::runtime_error("Failed to allocate memory pool");
        }
        
        // å»ºè®®ä½¿ç”¨å¤§é¡µé¢
        madvise(memory_start, total_size, MADV_HUGEPAGE);
        
        // åˆå§‹åŒ–ç©ºé—²é“¾è¡¨
        init_free_list(num_blocks);
    }
    
    void* allocate() {
        if (!free_list) {
            return nullptr;  // å†…å­˜æ± è€—å°½
        }
        
        Block* block = free_list;
        free_list = block->next;
        allocated_count.fetch_add(1, std::memory_order_relaxed);
        
        return block;
    }
    
    void deallocate(void* ptr) {
        if (!ptr) return;
        
        Block* block = static_cast<Block*>(ptr);
        block->next = free_list;
        free_list = block;
        allocated_count.fetch_sub(1, std::memory_order_relaxed);
    }
    
private:
    void init_free_list(size_t num_blocks) {
        char* current = static_cast<char*>(memory_start);
        free_list = reinterpret_cast<Block*>(current);
        
        for (size_t i = 0; i < num_blocks - 1; ++i) {
            Block* block = reinterpret_cast<Block*>(current);
            current += block_size;
            block->next = reinterpret_cast<Block*>(current);
        }
        
        // æœ€åä¸€ä¸ªå—
        reinterpret_cast<Block*>(current)->next = nullptr;
    }
    
public:
    ~HighPerformanceMemoryPool() {
        munmap(memory_start, total_size);
    }
};
```

### **2. NUMA æ„ŸçŸ¥å†…å­˜åˆ†é…**

```cpp
#include <numa.h>
#include <numaif.h>

class NUMAMemoryManager {
private:
    int num_nodes;
    std::vector<void*> node_memory;
    
public:
    NUMAMemoryManager() {
        if (numa_available() == -1) {
            throw std::runtime_error("NUMA not available");
        }
        
        num_nodes = numa_max_node() + 1;
        node_memory.resize(num_nodes, nullptr);
    }
    
    // åœ¨æŒ‡å®šNUMAèŠ‚ç‚¹åˆ†é…å†…å­˜
    void* allocate_on_node(size_t size, int node) {
        void* ptr = numa_alloc_onnode(size, node);
        if (!ptr) {
            throw std::runtime_error("Failed to allocate on NUMA node");
        }
        return ptr;
    }
    
    // åœ¨å½“å‰CPUæ‰€åœ¨NUMAèŠ‚ç‚¹åˆ†é…å†…å­˜
    void* allocate_local(size_t size) {
        int current_node = numa_node_of_cpu(sched_getcpu());
        return allocate_on_node(size, current_node);
    }
    
    // ç»‘å®šå†…å­˜åˆ°NUMAèŠ‚ç‚¹
    void bind_memory(void* ptr, size_t size, int node) {
        unsigned long nodemask = 1UL << node;
        mbind(ptr, size, MPOL_BIND, &nodemask, sizeof(nodemask) * 8, 0);
    }
    
    // è·å–å†…å­˜æ‰€åœ¨NUMAèŠ‚ç‚¹
    int get_memory_node(void* ptr) {
        int node;
        get_mempolicy(&node, nullptr, 0, ptr, MPOL_F_NODE | MPOL_F_ADDR);
        return node;
    }
    
    void deallocate(void* ptr, size_t size) {
        numa_free(ptr, size);
    }
};
```

### **3. ç¼“å­˜å‹å¥½çš„æ•°æ®ç»“æ„**

```cpp
// ç¼“å­˜è¡Œå¯¹é½çš„æ•°æ®ç»“æ„
struct alignas(64) CacheLinePadded {
    std::atomic<uint64_t> value{0};
    // å¡«å……åˆ°æ•´ä¸ªç¼“å­˜è¡Œï¼Œé¿å…false sharing
};

// ç´§å‡‘çš„ç»“æ„ä½“æ•°ç»„ - æé«˜ç¼“å­˜å±€éƒ¨æ€§
template<typename T>
class CacheFriendlyVector {
private:
    static constexpr size_t CACHE_LINE_SIZE = 64;
    static constexpr size_t ELEMENTS_PER_CACHE_LINE = CACHE_LINE_SIZE / sizeof(T);
    
    T* data;
    size_t size_;
    size_t capacity_;
    
public:
    CacheFriendlyVector(size_t initial_capacity) : size_(0) {
        // åˆ†é…ç¼“å­˜è¡Œå¯¹é½çš„å†…å­˜
        capacity_ = (initial_capacity + ELEMENTS_PER_CACHE_LINE - 1) & 
                   ~(ELEMENTS_PER_CACHE_LINE - 1);
        
        if (posix_memalign(reinterpret_cast<void**>(&data), CACHE_LINE_SIZE, 
                          capacity_ * sizeof(T)) != 0) {
            throw std::runtime_error("Failed to allocate aligned memory");
        }
        
        // é¢„å–å†…å­˜åˆ°ç¼“å­˜
        for (size_t i = 0; i < capacity_; i += ELEMENTS_PER_CACHE_LINE) {
            __builtin_prefetch(&data[i], 1, 3);  // å†™é¢„å–ï¼Œé«˜å±€éƒ¨æ€§
        }
    }
    
    // æ‰¹é‡æ“ä½œ - å……åˆ†åˆ©ç”¨ç¼“å­˜
    template<typename Func>
    void batch_process(Func&& func, size_t batch_size = ELEMENTS_PER_CACHE_LINE) {
        for (size_t i = 0; i < size_; i += batch_size) {
            size_t end = std::min(i + batch_size, size_);
            
            // é¢„å–ä¸‹ä¸€æ‰¹æ•°æ®
            if (i + batch_size < size_) {
                __builtin_prefetch(&data[i + batch_size], 0, 3);
            }
            
            // å¤„ç†å½“å‰æ‰¹
            for (size_t j = i; j < end; ++j) {
                func(data[j]);
            }
        }
    }
    
    ~CacheFriendlyVector() {
        free(data);
    }
};
```

---

## ğŸ¯ æ€§èƒ½è°ƒä¼˜æŠ€å·§

### **1. CPU äº²å’Œæ€§ä¸è°ƒåº¦ä¼˜åŒ–**

```cpp
#include <sched.h>
#include <pthread.h>

class CPUAffinityManager {
public:
    // ç»‘å®šå½“å‰çº¿ç¨‹åˆ°æŒ‡å®šCPUæ ¸å¿ƒ
    static void bind_to_cpu(int cpu_id) {
        cpu_set_t cpuset;
        CPU_ZERO(&cpuset);
        CPU_SET(cpu_id, &cpuset);
        
        if (pthread_setaffinity_np(pthread_self(), sizeof(cpuset), &cpuset) != 0) {
            throw std::runtime_error("Failed to set CPU affinity");
        }
    }
    
    // ç»‘å®šåˆ°å¤šä¸ªCPUæ ¸å¿ƒ
    static void bind_to_cpus(const std::vector<int>& cpu_ids) {
        cpu_set_t cpuset;
        CPU_ZERO(&cpuset);
        
        for (int cpu_id : cpu_ids) {
            CPU_SET(cpu_id, &cpuset);
        }
        
        pthread_setaffinity_np(pthread_self(), sizeof(cpuset), &cpuset);
    }
    
    // è®¾ç½®å®æ—¶è°ƒåº¦ç­–ç•¥
    static void set_realtime_priority(int priority = 50) {
        struct sched_param param;
        param.sched_priority = priority;
        
        if (sched_setscheduler(0, SCHED_FIFO, &param) != 0) {
            // éœ€è¦rootæƒé™ï¼Œé™çº§åˆ°SCHED_OTHER
            param.sched_priority = 0;
            sched_setscheduler(0, SCHED_OTHER, &param);
            setpriority(PRIO_PROCESS, 0, -20);  // æœ€é«˜niceå€¼
        }
    }
    
    // è·å–CPUæ‹“æ‰‘ä¿¡æ¯
    static std::vector<std::vector<int>> get_cpu_topology() {
        std::vector<std::vector<int>> topology;
        int num_cpus = sysconf(_SC_NPROCESSORS_ONLN);
        
        // æŒ‰NUMAèŠ‚ç‚¹åˆ†ç»„CPU
        for (int node = 0; node <= numa_max_node(); ++node) {
            std::vector<int> node_cpus;
            for (int cpu = 0; cpu < num_cpus; ++cpu) {
                if (numa_node_of_cpu(cpu) == node) {
                    node_cpus.push_back(cpu);
                }
            }
            if (!node_cpus.empty()) {
                topology.push_back(node_cpus);
            }
        }
        
        return topology;
    }
};
```

### **2. å†…å­˜å±éšœä¸ç¼“å­˜æ§åˆ¶**

```cpp
#include <emmintrin.h>  // SSE2

class MemoryBarrierOptimization {
public:
    // ç¼–è¯‘å™¨å±éšœ - é˜²æ­¢æŒ‡ä»¤é‡æ’
    static void compiler_barrier() {
        asm volatile("" ::: "memory");
    }
    
    // CPU å†…å­˜å±éšœ
    static void memory_fence() {
        std::atomic_thread_fence(std::memory_order_seq_cst);
    }
    
    // è½»é‡çº§è¯»å±éšœ
    static void read_barrier() {
        std::atomic_thread_fence(std::memory_order_acquire);
    }
    
    // è½»é‡çº§å†™å±éšœ
    static void write_barrier() {
        std::atomic_thread_fence(std::memory_order_release);
    }
    
    // ç¼“å­˜è¡Œåˆ·æ–°
    static void flush_cache_line(const void* addr) {
        _mm_clflush(addr);
    }
    
    // éä¸´æ—¶å­˜å‚¨ - ç»•è¿‡ç¼“å­˜
    static void non_temporal_store(void* addr, __m128i data) {
        _mm_stream_si128(static_cast<__m128i*>(addr), data);
    }
    
    // é¢„å–æ•°æ®åˆ°ç¼“å­˜
    template<int locality>
    static void prefetch(const void* addr) {
        static_assert(locality >= 0 && locality <= 3, "Locality must be 0-3");
        __builtin_prefetch(addr, 0, locality);
    }
    
    // é¢„å–ç”¨äºå†™å…¥
    template<int locality>
    static void prefetch_for_write(const void* addr) {
        static_assert(locality >= 0 && locality <= 3, "Locality must be 0-3");
        __builtin_prefetch(addr, 1, locality);
    }
};
```

### **3. åˆ†æ”¯é¢„æµ‹ä¼˜åŒ–**

```cpp
// åˆ†æ”¯é¢„æµ‹æç¤º
#define LIKELY(x)   __builtin_expect(!!(x), 1)
#define UNLIKELY(x) __builtin_expect(!!(x), 0)

class BranchOptimization {
public:
    // çƒ­è·¯å¾„ä¼˜åŒ–
    template<typename T>
    static T fast_max(T a, T b) {
        if (LIKELY(a > b)) {
            return a;
        } else {
            return b;
        }
    }
    
    // é”™è¯¯å¤„ç†è·¯å¾„ä¼˜åŒ–
    static bool validate_input(const void* ptr, size_t size) {
        if (UNLIKELY(!ptr)) {
            handle_null_pointer_error();
            return false;
        }
        
        if (UNLIKELY(size == 0)) {
            handle_zero_size_error();
            return false;
        }
        
        return true;
    }
    
    // å¾ªç¯å±•å¼€ä¼˜åŒ–
    static void optimized_copy(const char* src, char* dst, size_t size) {
        size_t i = 0;
        
        // æŒ‰8å­—èŠ‚å¯¹é½å¤åˆ¶
        for (; i + 8 <= size; i += 8) {
            *reinterpret_cast<uint64_t*>(dst + i) = 
                *reinterpret_cast<const uint64_t*>(src + i);
        }
        
        // å¤„ç†å‰©ä½™å­—èŠ‚
        for (; i < size; ++i) {
            dst[i] = src[i];
        }
    }
    
private:
    static void handle_null_pointer_error() { /* é”™è¯¯å¤„ç† */ }
    static void handle_zero_size_error() { /* é”™è¯¯å¤„ç† */ }
};
```

---

## ğŸ“Š å®æˆ˜æ¡ˆä¾‹ä¸åŸºå‡†æµ‹è¯•

### **1. é«˜æ€§èƒ½æœåŠ¡å™¨æ¶æ„**

```cpp
// ç”Ÿäº§çº§é«˜æ€§èƒ½æœåŠ¡å™¨
class HighPerformanceServer {
private:
    HighPerformanceEpoll epoll;
    std::vector<std::thread> worker_threads;
    SPSCLockFreeQueue<int, 4096> connection_queue;
    HighPerformanceMemoryPool memory_pool;
    
public:
    HighPerformanceServer(int num_workers = std::thread::hardware_concurrency()) 
        : memory_pool(4096, 10000) {  // 4KBå—ï¼Œ10000ä¸ª
        
        // å¯åŠ¨å·¥ä½œçº¿ç¨‹
        for (int i = 0; i < num_workers; ++i) {
            worker_threads.emplace_back([this, i]() {
                // ç»‘å®šCPU
                CPUAffinityManager::bind_to_cpu(i % sysconf(_SC_NPROCESSORS_ONLN));
                
                // è®¾ç½®å®æ—¶ä¼˜å…ˆçº§
                CPUAffinityManager::set_realtime_priority();
                
                // å·¥ä½œå¾ªç¯
                worker_loop();
            });
        }
    }
    
    void worker_loop() {
        while (true) {
            int client_fd;
            if (connection_queue.dequeue(client_fd)) {
                handle_client(client_fd);
            } else {
                // æ²¡æœ‰æ–°è¿æ¥ï¼Œè®©å‡ºCPU
                std::this_thread::yield();
            }
        }
    }
    
    void handle_client(int client_fd) {
        // ä½¿ç”¨å†…å­˜æ± åˆ†é…ç¼“å†²åŒº
        void* buffer = memory_pool.allocate();
        if (!buffer) {
            close(client_fd);
            return;
        }
        
        // å¤„ç†è¯·æ±‚
        ssize_t bytes_read = read(client_fd, buffer, 4096);
        if (bytes_read > 0) {
            // å¤„ç†æ•°æ®
            process_request(buffer, bytes_read);
            
            // å‘é€å“åº”
            send_response(client_fd, buffer);
        }
        
        // å½’è¿˜ç¼“å†²åŒº
        memory_pool.deallocate(buffer);
        close(client_fd);
    }
    
private:
    void process_request(void* data, size_t size) { /* ä¸šåŠ¡é€»è¾‘ */ }
    void send_response(int fd, void* data) { /* å‘é€å“åº” */ }
};
```

### **2. æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·**

```cpp
#include <chrono>
#include <iomanip>

class PerformanceBenchmark {
private:
    using Clock = std::chrono::high_resolution_clock;
    using TimePoint = Clock::time_point;
    
    TimePoint start_time;
    std::string test_name;
    
public:
    PerformanceBenchmark(const std::string& name) : test_name(name) {
        start_time = Clock::now();
    }
    
    ~PerformanceBenchmark() {
        auto end_time = Clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::microseconds>(
            end_time - start_time);
        
        std::cout << std::fixed << std::setprecision(2)
                  << "[" << test_name << "] è€—æ—¶: " << duration.count() 
                  << " å¾®ç§’" << std::endl;
    }
    
    // ååé‡æµ‹è¯•
    static void throughput_test(const std::string& name, 
                               std::function<void()> operation,
                               size_t iterations) {
        auto start = Clock::now();
        
        for (size_t i = 0; i < iterations; ++i) {
            operation();
        }
        
        auto end = Clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
        
        double ops_per_second = (iterations * 1000000.0) / duration.count();
        
        std::cout << std::fixed << std::setprecision(0)
                  << "[" << name << "] ååé‡: " << ops_per_second 
                  << " æ“ä½œ/ç§’" << std::endl;
    }
    
    // å»¶è¿Ÿæµ‹è¯•
    static void latency_test(const std::string& name,
                           std::function<void()> operation,
                           size_t iterations) {
        std::vector<uint64_t> latencies;
        latencies.reserve(iterations);
        
        for (size_t i = 0; i < iterations; ++i) {
            auto start = Clock::now();
            operation();
            auto end = Clock::now();
            
            latencies.push_back(
                std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count()
            );
        }
        
        std::sort(latencies.begin(), latencies.end());
        
        std::cout << "[" << name << "] å»¶è¿Ÿç»Ÿè®¡ (çº³ç§’):\n"
                  << "  æœ€å°å€¼: " << latencies[0] << "\n"
                  << "  P50: " << latencies[iterations / 2] << "\n"
                  << "  P90: " << latencies[iterations * 9 / 10] << "\n"
                  << "  P99: " << latencies[iterations * 99 / 100] << "\n"
                  << "  æœ€å¤§å€¼: " << latencies.back() << std::endl;
    }
};

// ä½¿ç”¨ç¤ºä¾‹
void run_performance_tests() {
    // æµ‹è¯•æ— é”é˜Ÿåˆ— vs äº’æ–¥é”é˜Ÿåˆ—
    SPSCLockFreeQueue<int, 1024> lockfree_queue;
    
    PerformanceBenchmark::throughput_test("æ— é”é˜Ÿåˆ—å…¥é˜Ÿ", [&]() {
        lockfree_queue.enqueue(42);
    }, 1000000);
    
    PerformanceBenchmark::throughput_test("æ— é”é˜Ÿåˆ—å‡ºé˜Ÿ", [&]() {
        int value;
        lockfree_queue.dequeue(value);
    }, 1000000);
    
    // æµ‹è¯•å†…å­˜åˆ†é…æ€§èƒ½
    HighPerformanceMemoryPool pool(64, 10000);
    
    PerformanceBenchmark::latency_test("å†…å­˜æ± åˆ†é…", [&]() {
        void* ptr = pool.allocate();
        pool.deallocate(ptr);
    }, 100000);
}
```

---

## ğŸš€ æ€»ç»“ä¸æœ€ä½³å®è·µ

### **æ€§èƒ½ä¼˜åŒ–åŸåˆ™**
1. **æµ‹é‡é©±åŠ¨ä¼˜åŒ–** - å…ˆæµ‹é‡ï¼Œå†ä¼˜åŒ–ï¼Œé¿å…è¿‡æ—©ä¼˜åŒ–
2. **çƒ­ç‚¹è·¯å¾„ä¼˜åŒ–** - ä¸“æ³¨äºå ç”¨CPUæ—¶é—´æœ€å¤šçš„ä»£ç è·¯å¾„
3. **ç¼“å­˜å‹å¥½è®¾è®¡** - æ•°æ®ç»“æ„å’Œç®—æ³•è¦è€ƒè™‘ç¼“å­˜å±€éƒ¨æ€§
4. **å‡å°‘ç³»ç»Ÿè°ƒç”¨** - æ‰¹é‡æ“ä½œï¼Œå‡å°‘ç”¨æˆ·æ€-å†…æ ¸æ€åˆ‡æ¢
5. **NUMAæ„ŸçŸ¥ç¼–ç¨‹** - åœ¨å¤šNUMAèŠ‚ç‚¹ç³»ç»Ÿä¸Šè€ƒè™‘å†…å­˜è®¿é—®æ¨¡å¼

### **å¹¶å‘è®¾è®¡æ¨¡å¼**
1. **ç”Ÿäº§è€…-æ¶ˆè´¹è€…** - ä½¿ç”¨æ— é”é˜Ÿåˆ—è§£è€¦ç”Ÿäº§å’Œæ¶ˆè´¹
2. **å·¥ä½œçªƒå–** - å¤šçº¿ç¨‹ä»»åŠ¡åˆ†å‘çš„é«˜æ•ˆæ¨¡å¼
3. **è¯»å†™åˆ†ç¦»** - ä½¿ç”¨RCUæˆ–è¯»å†™é”ä¼˜åŒ–è¯»å¤šå†™å°‘åœºæ™¯
4. **äº‹ä»¶é©±åŠ¨** - ä½¿ç”¨epoll/io_uringå®ç°é«˜å¹¶å‘I/O
5. **æ‰¹é‡å¤„ç†** - å‡å°‘é”ç«äº‰å’Œç³»ç»Ÿè°ƒç”¨å¼€é”€

### **è°ƒè¯•ä¸ç›‘æ§**
```bash
# æ€§èƒ½åˆ†æå·¥å…·
perf stat ./your_app                    # CPUæ€§èƒ½ç»Ÿè®¡
perf record -g ./your_app              # æ€§èƒ½çƒ­ç‚¹åˆ†æ
perf mem record ./your_app             # å†…å­˜è®¿é—®åˆ†æ

# ç³»ç»Ÿç›‘æ§
top -H                                 # æŸ¥çœ‹çº¿ç¨‹CPUä½¿ç”¨
iostat                                 # I/Oç»Ÿè®¡
numastat                              # NUMAå†…å­˜ç»Ÿè®¡
```

Linuxé«˜æ€§èƒ½å¹¶å‘ç¼–ç¨‹éœ€è¦æ·±å…¥ç†è§£ç¡¬ä»¶ç‰¹æ€§ã€æ“ä½œç³»ç»Ÿæœºåˆ¶å’Œç¼–ç¨‹è¯­è¨€ç‰¹æ€§ã€‚é€‰æ‹©åˆé€‚çš„å¹¶å‘æ¨¡å‹å’Œä¼˜åŒ–æŠ€æœ¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡åº”ç”¨æ€§èƒ½ï¼
